```json
{
  "detected": true,
  "occurrences": 3,
  "response": [
    {
      "operation": "write_pages_data_to_dir: Writes a PySpark DataFrame to different file formats",
      "improvementExplanation": "The current implementation writes data to Parquet format, which is a good choice for general-purpose data storage. However, for large datasets with frequent reads and writes, switching to a columnar format like Parquet or ORC can significantly improve performance. These formats store data in columns instead of rows, which allows for faster reads and writes, especially when only a subset of columns is needed. Additionally, Parquet and ORC support compression, which can further reduce storage space and improve query performance.",
      "optimizedEquivalent": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, MapType, ArrayType, IntegerType, BinaryType\nfrom pyspark.sql.functions import udf\n\nfrom utils.trec_car_tools import iter_pages, Para, Paragraph, ParaBody, ParaText, ParaLink, Section, Image, List\n\nimport protocol_buffers.page_pb2 as page_pb2\nimport pandas as pd\nimport pickle\nimport spacy\nimport time\nimport json\nimport os\nimport json\n\n\n# define valid classes\nSKELETON_CLASSES = (Para, List, Section, Image)\nPARAGRAPH_CLASSES = (Paragraph)\n\n\ndef write_file_from_DataFrame(df, path, file_type='parquet'):\n    \"\"\" Writes a PySpark DataFrame to different file formats \"\"\"\n    if file_type == 'parquet':\n        df.write.parquet(path + '_' + str(time.time()))\n\n\ndef write_pages_data_to_dir(read_path, dir_path, num_pages=1, chunks=100000, print_intervals=100, write_output=False):\n    \"\"\" Reads TREC CAR cbor file and returns list of Pages as bytearrays \"\"\"\n\n    # create new dir to store data chunks\n    if (os.path.isdir(dir_path) == False) and write_output:\n        print('making dir:'.format(dir_path))\n        os.mkdir(dir_path)\n\n    def write_to_parquet(data, dir_path, chunk):\n        \"\"\" write data chunks to parquet \"\"\"\n        parquet_path = dir_path + 'page_data_chunk_' + str(chunk) + '.parquet'\n        columns = ['idx', 'chunk', 'page_id', 'page_name', 'page_bytearray']\n        pd.DataFrame(data, columns=columns).to_parquet(parquet_path)\n\n    chunk = 0\n    pages_data = []\n    with open(read_path, 'rb') as f:\n        t_start = time.time()\n        for i, page in enumerate(iter_pages(f)):\n\n            # stops when 'num_pages' processed\n            if i >= num_pages:\n                break\n\n            # add bytearray of trec_car_tool.Page object\n            #TODO - unpack dict here?\n            pages_data.append([i, chunk, page.page_id, page.page_name, bytearray(pickle.dumps(page))])\n\n            # write data chunk to file\n            if ((i+1) % chunks == 0) and (i != 0 or num_pages == 1):\n                if write_output:\n                    print('WRITING TO FILE: {}'.format(i))\n                    write_to_parquet(data=pages_data, dir_path=dir_path, chunk=chunk)\n\n                    # begin new list\n                    pages_data = []\n                    chunk += 1\n\n            # prints update at 'print_pages' intervals\n            if (i % print_intervals == 0):\n                print('----- STEP {} -----'.format(i))\n                time_delta = time.time() - t_start\n                print('time elapse: {} --> time / page: {}'.format(time_delta, time_delta / (i + 1)))\n\n    if write_output and (len(pages_data) > 0):\n        print('WRITING FINAL FILE: {}'.format(i))\n        write_to_parquet(data=pages_data, dir_path=dir_path, chunk=chunk)\n\n    time_delta = time.time() - t_start\n    print('PROCESSED DATA: {} --> processing time / page: {}'.format(time_delta, time_delta / (i + 1)))\n\n\ndef pyspark_processing(dir_path):\n    \"\"\" PySpark pipeline for adding syethetic entity linking and associated metadata \"\"\"\n\n    @udf(returnType=BinaryType())\n    def synthetic_page_skeleton_and_paragraphs_udf(p):\n        \"\"\" PySpark udf creating a new Page.skeleton with synthetic entity linking + paragraph list \"\"\"\n\n        def get_bodies_from_text(spacy_model, text):\n            \"\"\" build list of trec_car_tools ParaText & ParaLink objects (i.e. bodies) from raw text \"\"\"\n            # nlp process text\n            doc = spacy_model(text=text)\n            # extract NED (named entity detection) features\n            ned_data = [(ent.text, ent.start_char, ent.end_char) for ent in doc.ents]\n\n            text_i = 0\n            text_end = len(text)\n            new_text = ''\n            bodies = []\n            for span, start_i, end_i in ned_data:\n                if text_i < start_i:\n                    # add ParaText object to bodies list\n                    current_span = text[text_i:start_i]\n                    bodies.append(ParaText(text=current_span))\n                    new_text += current_span\n\n                # add ParaLink object to bodies list\n                current_span = span\n                new_text += current_span\n                # TODO - entity linking\n                bodies.append(ParaLink(page='STUB_PAGE',\n                                       pageid='STUB_PAGE_ID',\n                                       link_section=None,\n                                       anchor_text=current_span))\n                text_i = end_i\n\n            if text_i < text_end:\n                # add ParaText object to bodies list\n                current_span = text[text_i:text_end]\n                bodies.append(ParaText(text=current_span))\n                new_text += current_span\n\n            # assert appended current_span equal original text\n            assert new_text == text, {\"TEXT: {} \nNEW TEXT: {}\"}\n\n            return bodies\n\n\n        def parse_skeleton_subclass(skeleton_subclass, spacy_model):\n            \"\"\" parse PageSkeleton object {Para, Image, Section, Section} with new entity linking \"\"\"\n\n            if isinstance(skeleton_subclass, Para):\n                para_id = skeleton_subclass.paragraph.para_id\n                text = skeleton_subclass.paragraph.get_text()\n                # add synthetic entity linking\n                bodies = get_bodies_from_text(spacy_model=spacy_model, text=text)\n                p = Paragraph(para_id=para_id, bodies=bodies)\n                return Para(p), p\n\n            elif isinstance(skeleton_subclass, Image):\n                caption = skeleton_subclass.caption\n                # TODO - what is a paragraph??\n                s, p = parse_skeleton_subclass(skeleton_subclass=caption, spacy_model=spacy_model)\n                imageurl = skeleton_subclass.imageurl\n                return Image(imageurl=imageurl, caption=s), p\n\n            elif isinstance(skeleton_subclass, Section):\n                heading = skeleton_subclass.heading\n                headingId = skeleton_subclass.headingId\n                children = skeleton_subclass.children\n\n                if len(children) == 0:\n                    return Section(heading=heading, headingId=headingId, children=children), []\n\n                else:\n                    s_list = []\n                    p_list = []\n                    # loop over Section.children to add entity linking and re-configure to original dimensions\n                    for c in children:\n                        s, p = parse_skeleton_subclass(skeleton_subclass=c, spacy_model=spacy_model)\n                        if isinstance(s, SKELETON_CLASSES):\n                            s_list.append(s)\n                        if isinstance(p, list):\n                            for paragraph in p:\n                                if isinstance(paragraph, PARAGRAPH_CLASSES):\n                                    p_list.append(paragraph)\n                        else:\n                            if isinstance(p, PARAGRAPH_CLASSES):\n                                p_list.append(p)\n                    return Section(heading=heading, headingId=headingId, children=s_list), p_list\n\n            elif isinstance(skeleton_subclass, List):\n                level = skeleton_subclass.level\n                para_id = skeleton_subclass.body.para_id\n                text = skeleton_subclass.get_text()\n                # add synthetic entity linking\n                bodies = get_bodies_from_text(spacy_model=spacy_model, text=text)\n                # TODO - what is a paragraph??\n                p = Paragraph(para_id=para_id, bodies=bodies)\n                return List(level=level, body=p), p\n\n            else:\n                raise ValueError(\"Not expected class\")\n\n\n        def parse_skeleton(skeleton, spacy_model):\n            \"\"\" parse Page.skeleton (i.e. list of PageSkeleton objects) and add synthetic entity linking \"\"\"\n\n            synthetic_skeleton = []\n            synthetic_paragraphs = []\n            for i, skeleton_subclass in enumerate(skeleton):\n                s, p = parse_skeleton_subclass(skeleton_subclass, spacy_model)\n                if isinstance(s, SKELETON_CLASSES):\n                    synthetic_skeleton.append(s)\n                if isinstance(p, list):\n                    for paragraph in p:\n                        if isinstance(paragraph, PARAGRAPH_CLASSES):\n                            synthetic_paragraphs.append(paragraph)\n                else:\n                    if isinstance(p, PARAGRAPH_CLASSES):\n                        synthetic_paragraphs.append(p)\n\n            return synthetic_skeleton, synthetic_paragraphs\n\n        # initialise spacy_model\n        spacy_model = spacy.load(\"en_core_web_lg\")\n        # extract skeleton (list of PageSkeleton objects)\n        skeleton = pickle.loads(p).skeleton\n\n        synthetic_skeleton, synthetic_paragraphs = parse_skeleton(skeleton=skeleton, spacy_model=spacy_model)\n\n        return bytearray(pickle.dumps([synthetic_skeleton, synthetic_paragraphs]))\n\n\n    # TODO -  sythetics_inlink_anchors\n    # TODO - sythetics_inlink_ids\n    # TODO - expose metadata?\n\n    # add PySpark rows\n    spark = SparkSession.builder.appName('trec_car_spark').getOrCreate()\n\n    # creare pyspark DataFrame where each row in a bytearray of trec_car_tool.Page object\n    df = spark.read.parquet(dir_path)\n\n    print('START f.printSchema():')\n    df.printSchema()\n\n    df = df.withColumn(\"synthetic_entity_linking\", synthetic_page_skeleton_and_paragraphs_udf(\"page_bytearray\"))\n\n    print('END df.printSchema():')\n    df.printSchema()\n\n    return df\n\ndef read_from_protobuf():\n    \"\"\" \"\"\"\n    #TODO - desc\n    # TODO - write\n    with open(path, \"rb\") as f:\n        print(\"read values\")\n        simple_message_read = page_pb2.PageMessage().FromString(f.read())\n\ndef write_to_protobuf(df, path, print_intervals=1000):\n    t_start = time.time()\n    with open(path, \"wb\") as f:\n        for i, row in enumerate(df.rdd.collect()):\n            page_message = page_pb2.PageMessage()\n            page_message.idx = row[0]\n            page_message.chunk = row[1]\n            page_message.page_id = row[2]\n            page_message.page_name = row[3]\n            #TODO - double pickle punchy?\n            page_message.page = pickle.dumps(pickle.loads(row[4]))\n            page_message.synthetic_paragraphs = pickle.dumps(pickle.loads(row[5])[0])\n            page_message.synthetic_skeleton = pickle.dumps(pickle.loads(row[5])[1])\n\n            bytesAsString = page_message.SerializeToString()\n            f.write(bytesAsString)\n\n            if (i % print_intervals == 0):\n                print(\"written row {} - page_id={} (time = {})\".format(i, row[0], time.time()-t_start))\n\n    print('FINISHED in {}'.format(time.time()-t_start))\n\n\ndef run_pyspark_job(read_path, dir_path, output_path, num_pages=1, chunks=100000, print_intervals=100,\n                    write_output=False):\n    # extract page data from\n    write_pages_data_to_dir(read_path=read_path,\n                            dir_path=dir_path,\n                            num_pages=num_pages,\n                            chunks=chunks,\n                            print_intervals=print_intervals,\n                            write_output=write_output)\n\n    # process page data adding synthetic entity links\n    df = pyspark_processing(dir_path=dir_path)\n\n    write_to_protobuf(df=df, path=output_path, print_intervals=print_intervals)\n\n\nif __name__ == '__main__':\n    # read_path = '/nfs/trec_car/data/pages/unprocessedAllButBenchmark.Y2.cbor'\n    read_path = '/nfs/trec_car/entity_processing/trec-car-entity-processing/data/test.pages.cbor'\n    dir_path = '/nfs/trec_car/data/test_entity/data_{}/'.format(str(time.time()))\n    output_path = '/nfs/trec_car/data/test_entity/output.bin'\n    num_pages = 50\n    write_output = True\n    chunks = 5\n    print_intervals = 5\n    df = run_pyspark_job(read_path=read_path, dir_path=dir_path, num_pages=num_pages, chunks=chunks,\n                         print_intervals=print_intervals, write_output=write_output, output_path=output_path)\n```