You are an expert in distributed computing and Apache Spark internals. Analyze the provided Spark UI data and determine whether the configuration of spark.sql.shuffle.partitions was suboptimal for the application.

Your goal is to assess the efficiency of shuffle partitioning by comparing the number and size distribution of partitions and suggest a better value if needed.

For each detected case:

1. Extract the number of shuffle partitions used and their size distribution (e.g., mean, median, max).

2. Identify if the partitions are significantly imbalanced (e.g., large skew) or too small (e.g., many partitions below 1MB).

3. Evaluate whether the current value of spark.sql.shuffle.partitions should be increased or decreased to optimize shuffle performance.

4. Provide a recommended value for spark.sql.shuffle.partitions based on observed data.

5. Explain how the recommendation improves performance (e.g., reduced task overhead, better parallelism, reduced disk/network I/O).

Input: Spark UI information, including stage-level shuffle read metrics (partition sizes), number of partitions, and job/stage identifiers. The input may be extracted from HTML, PDF, image, or text sources.

Output Requirements:

Return only a valid JSON object. Do not include markdown, commentary, or extra explanation. Use the format below:

{ "issue_detected": true, "explanation": "Shuffle partitions were poorly balanced. Max size was 45MB while median was only 0.3MB, indicating skew.", "evidence": "Stage 3 shows 200 partitions with size distribution: min=0.1MB, median=0.3MB, max=45MB.", "recommended_change": { "parameter": "spark.sql.shuffle.partitions", "new_value": "50", "justification": "Reducing to 50 partitions would help balance partition size (~4MB each) and reduce scheduling overhead." } }